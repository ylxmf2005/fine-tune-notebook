{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ethan\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\ethan\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ethan\\.cache\\huggingface\\hub\\models--uer--gpt2-distil-chinese-cluecorpussmall. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '中 国 是 一 个 伟 大 的 国 家 吗 ？ 看 到 知 友 质 疑 有 评 论 说 我 错 了 ， 我 说 是 ， 我 又 是 来 自 于 父 国 的 反 华 势 力 。 我 的 祖 国 是 我 的 国 。 我 没 有 歧 视 祖 国 ， 我 说 国 也 是 我 的 国 。 但 是 我 可 以 反 驳 我 的 答 案 。 我 不 能 因 为 祖 国 只 是 一 个 国 家 就 否 定 祖 国 是 对 我 自 己 的 错 ？ 为 什 么 中 国 会 被 我 这 个 回 答 误 导 了 ？ 不 能 因 为 祖 国 只 是 一 个 国 家 。 祖 国 只 是 一 个 国 家 吗 ？ 这 样 的 问 题 ， 我 没 法 回 答 。 你 想 了 想 ， 如 果 你 觉 得 很 好 的 话 ， 请 转 告 给 我 的 朋 友 。 我 只 想 你 的 答 案'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HuggingFace-transformers\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "\n",
    "# 实例化tokenizer和model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)\n",
    "text_generator(\"中 国 是 一 个 伟 大 的 国 家 吗 ？\", max_length=200, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset  # 数据封装 (将文本数据封装为可以分批次拿来训练的张量)\n",
    "from transformers import DataCollatorForLanguageModeling  # 数据整理\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 实例化tokenizer和model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "max_length = 300\n",
    "\n",
    "def split_text_into_chunks(text, max_len=512):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    for paragraph in paragraphs:        \n",
    "        tokens = tokenizer.tokenize(paragraph)\n",
    "        if len(tokens) > max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "            paragraph = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "        combined_chunk = current_chunk + paragraph + \"  \"\n",
    "        combined_tokens = tokenizer.tokenize(combined_chunk)\n",
    "        \n",
    "        if len(combined_tokens) <= max_len:\n",
    "            current_chunk = combined_chunk\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = paragraph + \"  \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "        \n",
    "\n",
    "sentences = []\n",
    "data_path = r'./data/wiki_zh_2019/wiki_00'\n",
    "import json\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        sentences.extend(split_text_into_chunks(text, max_len=max_length))\n",
    "\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用tokenizer处理文本\n",
    "encoding = tokenizer(sentences,\n",
    "                     max_length=max_length,\n",
    "                     padding='max_length',\n",
    "                     truncation=True,\n",
    "                     return_tensors='pt')\n",
    "# return_tensors 参数还可以设置为以下值：\n",
    "# 'tf' - 返回 TensorFlow 张量\n",
    "# 'np' - 返回 NumPy 数组\n",
    "# None - 返回 Python 列表（默认值）\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    'input_ids': encoding['input_ids'],  # 输入数据的 token 序列，通常通过分词器生成\n",
    "    'attention_mask': encoding['attention_mask']  \n",
    "    # 为了让模型能够处理不同长度的输入，通常会将输入填充到相同长度，但我们不希望模型关注这些填充的部分。\n",
    "    # [CLS] 我 爱 自然 语言 处理 [SEP] [PAD] [PAD] [PAD]\n",
    "    # attention_mask = [1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "}\n",
    "train_dataset = Dataset.from_dict(data_dict)  \n",
    "\n",
    "# 数据整理\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  # mlm 表示是否进行掩码语言模型训练 (Masked Language Model, MLM)\n",
    "    # 当 mlm=True 时，MLM 会随机掩盖部分输入 token，模型任务是预测被掩盖的 token，\n",
    "    # 适用于 BERT 等模型。mlm=False 则不进行掩盖，通常适用于 GPT 等非掩码模型。\n",
    ")\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='./output',  # 训练输出的目录，存储模型的权重、配置文件等\n",
    "    overwrite_output_dir=True,  # 是否覆盖之前的输出文件，如果设为 False，训练会在发现已有输出文件时报错\n",
    "    num_train_epochs=5,  # 训练的轮次\n",
    "    per_device_train_batch_size=32,  # 每个设备（如 GPU）的训练批次大小，这里是 32。较大的 batch size 可能会加速训练，但需要更多显存\n",
    "    save_steps=3000,  # 每隔 3000 步保存模型\n",
    "    save_total_limit=2,  # 只保存最新的 2 个模型检查点，较大的值会保留更多模型版本，但会占用更多存储空间\n",
    "    warmup_steps=50,   # 训练开始时的学习率预热步数，在训练的初始阶段，学习率会从一个较小的值逐渐增加到预设的学习率。这种预热策略可以帮助模型在训练初期避免梯度爆炸，更稳定。\n",
    "    logging_dir='./logs',  # 日志文件存放目录\n",
    "    logging_steps=200,  # 每 200 步记录一次日志\n",
    "    report_to=[]  # 指定将训练过程报告到哪里，比如 `report_to=['tensorboard']` 用于可视化    `report_to=[\"wandb\"]` 将训练过程报告到 Weights & Biases (W&B) 平台\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = train_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
